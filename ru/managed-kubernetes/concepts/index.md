---
title: "Взаимосвязь ресурсов сервиса {{ k8s }}"
description: "Основная сущность, которой оперирует сервис, — кластер {{ k8s }}. Кластер {{ k8s }} состоит из мастера и одной или нескольких групп узлов. Мастер отвечает за управление кластером {{ k8s }}. На узлах запускаются контейнеризованные приложения пользователя."
---

# Взаимосвязь ресурсов в {{ managed-k8s-name }}

[{{ k8s }}](https://kubernetes.io/ru/) — система для управления [контейнерными приложениями](../../glossary/containerization.md#containers-apps). {{ k8s }} предоставляет механизмы взаимодействия с [кластером](../../glossary/cluster.md), с помощью которых осуществляется автоматизация развертывания, масштабирования и управления приложениями в контейнерах.

Основная сущность, которой оперирует сервис, — _кластер {{ k8s }}_.

## Кластер {{ k8s }} {#kubernetes-cluster}

Кластер {{ k8s }} состоит из мастера и одной или нескольких групп узлов. Мастер отвечает за управление кластером {{ k8s }}. На узлах запускаются контейнеризованные приложения пользователя.

Сервис полностью управляет мастером, а также следит за состоянием и работоспособностью группы узлов. Пользователь может управлять узлами напрямую, а также настраивать кластер {{ k8s }} с помощью консоли управления {{ yandex-cloud }}, CLI и API {{ managed-k8s-name }}.

{% include [Install kubectl](../../_includes/managed-kubernetes/note-node-group-internet-access.md) %}

При работе с кластером {{ k8s }} на инфраструктуре {{ yandex-cloud }} задействуются следующие ресурсы:

Ресурс | Количество | Комментарий
--- | --- | ---
Подсеть | 2 | {{ k8s }} резервирует диапазоны IP-адресов, которые будут использоваться для подов и сервисов.
Публичный IP | N | В количество N входит:<br>* **Один** публичный IP для NAT-инстанса.<br>* Публичный IP **каждому** узлу в группе, если вы используете технологию one-to-one NAT.

## Мастер {#master}

_Мастер_ — компонент, который управляет кластером {{ k8s }}.

Мастер запускает управляющие процессы {{ k8s }}, которые включают сервер {{ k8s }} API, планировщик и контроллеры основных ресурсов. Жизненный цикл мастера управляется сервисом при создании или удалении кластера {{ k8s }}. Мастер отвечает за глобальные решения, которые выполняются на всех узлах кластера {{ k8s }}. Они включают в себя планирование рабочих нагрузок, таких как контейнерные приложения, управление жизненным циклом рабочих нагрузок и масштабированием.

Мастер бывает двух типов, которые отличаются расположением в [зонах доступности](../../overview/concepts/geo-scope.md):
* _Зональный_ — создается в подсети в одной зоне доступности.
* _Региональный_ — создается распределенно в трех подсетях в каждой зоне доступности. При недоступности одной зоны региональный мастер остается работоспособным.

  {% note warning %}

  Внутренний IP-адрес регионального мастера доступен только в пределах одной облачной сети {{ vpc-full-name }}.

  {% endnote %}

## Группа узлов {#node-group}

_Группа узлов_ — группа ВМ с одинаковой конфигурацией в кластере {{ k8s }}, на которых запускаются пользовательские контейнеры.

### Конфигурация {#config}

При создании группы узлов вы можете сконфигурировать следующие параметры ВМ:
* Тип ВМ.
* Тип и количество ядер (vCPU).
* Объем памяти (RAM) и диска.
* Параметры ядра.
  * _Безопасные_ (safe) параметры ядра изолированы между подами.
  * _Небезопасные_ (unsafe) параметры влияют на работу не только подов, но и узла в целом. В {{ managed-k8s-name }} нельзя изменять небезопасные параметры ядра, имена которых не были явно указаны при [создании группы узлов](../operations/node-group/node-group-create.md).

  {% note info %}

  Указывайте только параметры ядра, которые принадлежат [пространствам имен](#namespace), например, `net.ipv4.ping_group_range`. Параметры, которые не принадлежат пространствам имен, например, `vm.max_map_count`, необходимо разрешать непосредственно в операционной системе или при помощи DaemonSet с контейнерами в привилегированном режиме после создания [группы узлов {{ managed-k8s-name }}](#node-group).

  {% endnote %}

  Подробнее о параметрах ядра см. в [документации {{ k8s }}](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/).

В одном кластере {{ k8s }} можно создавать группы с разными конфигурациями и размещать их в разных зонах доступности.

Для {{ managed-k8s-name }} доступны следующие среды запуска контейнеров:
* [Платформа `Docker`](https://www.docker.com/) — выбирается по умолчанию при создании группы узлов.
* [Платформа `containerd`](https://containerd.io/) — может быть выбрана при создании или изменении группы узлов с {{ k8s }} версии 1.19 и выше.

### Подключение к узлам группы {#node-connect-ssh}

К узлам группы можно подключаться следующими способами:
* через SSH-клиент с помощью стандартной пары SSH-ключей, см. [{#T}](../operations/node-connect-ssh.md);
* через SSH-клиент и YC CLI с помощью OS Login, см. [{#T}](../operations/node-connect-oslogin.md).

### Политики taints и tolerations {#taints-tolerations}

_Taints_ — это особые политики, которые присваиваются узлам в группе. С помощью taint-политик можно запретить некоторым подам выполняться на определенных узлах. Например, можно указать, что поды для рендеринга должны запускаться только на [узлах с GPU](node-group/node-group-gpu.md).

Преимущества использования taint-политик:
* политики сохраняются, когда узел перезапускается или заменяется новым;
* при добавлении узлов в группу политики назначаются этому узлу автоматически;
* политики автоматически назначаются новым узлам при [масштабировании группы узлов](autoscale.md).

Назначить taint-политику для группы узлов можно только при ее [создании](../operations/node-group/node-group-create.md).

Каждая taint-политика состоит из трех частей:

```text
<ключ> = <значение>:<эффект>
```

Список доступных taint-эффектов:
* `NO_SCHEDULE` — запретить запуск новых подов на узлах группы (уже запущенные поды продолжат работу);
* `PREFER_NO_SCHEDULE` — избегать запуска подов на узлах группы, если для запуска этих подов есть свободные ресурсы в других группах;
* `NO_EXECUTE` — завершить работу подов на узлах этой группы, расселить их в другие группы, а запуск новых подов запретить.

_Tolerations_ — это исключения из taint-политик. С помощью tolerations можно разрешить определенным подам работать на узлах, даже если taint-политика группы узлов препятствует этому.

Например, если для узлов в группе настроена taint-политика `key1=value1:NoSchedule`, разместить поды на таком узле можно с помощью tolerations:

```yaml
apiVersion: v1
kind: Pod
...
spec:
  ...
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoSchedule"
```

{% note info %}

Для системных подов автоматически назначаются tolerations, позволяющие им работать на всех доступных узлах.

{% endnote %}

Подробнее о taint-политиках и исключениях см. в [документации {{ k8s }}](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/).

### Метки узлов {#node-labels}

_Метки узлов_ — механизм группировки узлов в {{ managed-k8s-name }}. Существуют различные виды меток:

* [Облачные метки группы узлов](../../resource-manager/concepts/labels.md) — используются для логического разделения и маркировки ресурсов. Например, с помощью облачных меток можно [следить за расходами](../../billing/operations/get-folder-report.md#format) на различные группы узлов. Обозначаются в CLI как `template-labels` и в {{ TF }} — как `labels`.

* [{{ k8s }}-метки узлов]({{ k8s-docs }}/concepts/overview/working-with-objects/labels/) — используются для группировки объектов {{ k8s }} и [распределения подов по узлам кластера](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes). Обозначаются в CLI как `node-labels` и в {{ TF }} — как `node_labels`.

  При назначении {{ k8s }}-меток указывайте характеристики узлов, по которым вы будете группировать объекты. Примеры {{ k8s }}-меток см. в [документации {{ k8s }}]({{ k8s-docs }}/concepts/overview/working-with-objects/labels/#причины-использования).

Оба вида меток могут использоваться одновременно, например, при [создании группы узлов](../operations/node-group/node-group-create.md) через CLI или {{ TF }}.

Для управления {{ k8s }}-метками доступны [API {{ managed-k8s-name }}](../api-ref/index.md) и [API {{ k8s }}]({{ k8s-docs }}/concepts/overview/kubernetes-api). Их особенности:

* {{ k8s }}-метки, добавленные через API {{ k8s }}, могут быть потеряны, так как во время [обновления или изменения групп узлов](../operations/node-group/node-group-update.md) часть узлов пересоздается с другим именем, а часть старых — удаляется.
* Если {{ k8s }}-метки созданы через API {{ managed-k8s-name }}, их не получится удалить через API {{ k8s }}. Иначе метки будут восстановлены после удаления.

{% note warning %}

Чтобы избежать потери меток, используйте API {{ managed-k8s-name }}.

{% endnote %}

Набор {{ k8s }}-меток `ключ: значение` может быть определен для каждого объекта. Для него все ключи должны быть уникальными.

Ключи {{ k8s }}-меток могут состоять из двух частей: префикса и имени, которые разделены знаком `/`.

Префикс — необязательная часть ключа. Требования к префиксу:
* Должен быть поддоменом DNS — серия DNS-меток, разделенных точками `.`.
* Длина — до 253 символов.
* За последним символом — `/`.

Имя — обязательная часть ключа. Требования к имени:
* Длина — до 63 символов.
* Может содержать строчные буквы латинского алфавита, цифры, дефисы, нижние подчеркивания и точки.
* Первый и последний символы — буква или цифра.

Об управлении {{ k8s }}-метками узлов читайте в разделе [{#T}](../operations/node-group/node-label-management.md).

## Под {#pod}

_Под_ — запрос на запуск одного или более контейнеров на одном узле группы. В рамках кластера {{ k8s }} каждый под имеет уникальный IP-адрес, чтобы приложения не конфликтовали при использовании портов.

Контейнеры описываются в поде через объект, написанный на языке JSON или YAML.

### Маскарадинг IP-адресов подов {#pod-ip-masquerade}

Если поду требуется доступ к ресурсам за пределами кластера, его IP-адрес будет заменен на IP-адрес узла, на котором работает под. Для этого в кластере используется механизм [маскарадинга IP-адресов](https://kubernetes.io/docs/tasks/administer-cluster/ip-masq-agent/).

По умолчанию, маскарадинг включен для всего диапазона IP-адресов подов.

Для реализации механизма маскарадинга на каждом узле кластера развернут под `ip-masq-agent`. Настройки этого пода хранятся в объекте ConfigMap с именем `ip-masq-agent`. Если необходимо отключить маскарадинг IP-адресов подов, например, для доступа к ним через [VPN](../../glossary/vpn.md) или [{{ interconnect-full-name }}](../../interconnect/index.yaml), укажите в параметре `data.config.nonMasqueradeCIDRs` нужные диапазоны IP-адресов:

```yaml
...
data:
  config: |+
    nonMasqueradeCIDRs:
      - <CIDR_IP-адресов_подов_для_которых_не_требуется_маскирование>
...
```

## Сервис {#service}

[_Сервис_](service.md) — абстракция, которая обеспечивает функции сетевой балансировки нагрузки. Правила подачи трафика настраиваются для группы подов, объединенных набором меток.

По умолчанию сервис доступен только внутри конкретного кластера {{ k8s }}, но может быть общедоступным и получать [запросы извне](../operations/create-load-balancer.md#lb-create) кластера {{ k8s }}.

## Пространство имен {#namespace}

_Пространство имен_ — абстракция, которая логически изолирует ресурсы кластера {{ k8s }} и распределяет [квоты]({{ link-console-quotas }}) на них. Это полезно для разделения ресурсов разных команд и проектов в одном кластере {{ k8s }}.

## Сервисные аккаунты {#service-accounts}

В кластерах {{ managed-k8s-name }} используется два типа сервисных аккаунтов:
* **Облачные сервисные аккаунты**

  Эти аккаунты существуют на уровне отдельного каталога в облаке и могут использоваться как {{ managed-k8s-name }}, так и другими сервисами.

  Подробнее см. в разделе [{#T}](../security/index.md) и [{#T}](../../iam/concepts/users/service-accounts.md).
* **Сервисные аккаунты {{ k8s }}**

  Эти аккаунты существуют и действуют только на уровне отдельного кластера {{ managed-k8s-name }}. Они применяются {{ k8s }} для:
  * Аутентификации запросов к API кластера от приложений, развернутых в кластере.
  * Настройки прав доступа для этих приложений.

  Набор сервисных аккаунтов {{ k8s }} автоматически создается в пространстве имен `kube-system` при развертывании кластера {{ managed-k8s-name }}.

  Для аутентификации внутри кластера {{ k8s }}, к которому относится сервисный аккаунт, создайте токен этого аккаунта вручную.

  Подробнее см. в [{#T}](../operations/connect/create-static-conf.md), а также в [документации {{ k8s }}](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/).

{% note warning %}

Не путайте [облачные сервисные аккаунты](../security/index.md#sa-annotation) и сервисные аккаунты {{ k8s }}.

В документации сервиса под _сервисным аккаунтом_ понимается облачный сервисный аккаунт, если не указано иное.

{% endnote %}

## Статистика кластера {{ managed-k8s-name }} {#metrics}

{% include [metrics-resources-list](../../_includes/managed-kubernetes/metrics-resources-list.md) %}

{% include [metrics-k8s-tools](../../_includes/managed-kubernetes/metrics-k8s-tools.md) %}

Описание метрик приводится в разделе [{#T}](../../managed-kubernetes/metrics.md).
